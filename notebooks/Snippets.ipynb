{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T02:15:40.052112Z",
     "iopub.status.busy": "2021-07-20T02:15:40.051845Z",
     "iopub.status.idle": "2021-07-20T02:15:40.063099Z",
     "shell.execute_reply": "2021-07-20T02:15:40.062455Z",
     "shell.execute_reply.started": "2021-07-20T02:15:40.052095Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import jsonlines\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# history storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T18:39:59.703915Z",
     "iopub.status.busy": "2021-07-16T18:39:59.703212Z",
     "iopub.status.idle": "2021-07-16T18:40:01.800582Z",
     "shell.execute_reply": "2021-07-16T18:40:01.799936Z",
     "shell.execute_reply.started": "2021-07-16T18:39:59.703727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /home/leonid/miniconda3/lib/python3.8/site-packages (2.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/leonid/miniconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T18:40:31.115996Z",
     "iopub.status.busy": "2021-07-16T18:40:31.115465Z",
     "iopub.status.idle": "2021-07-16T18:40:31.131787Z",
     "shell.execute_reply": "2021-07-16T18:40:31.131321Z",
     "shell.execute_reply.started": "2021-07-16T18:40:31.115931Z"
    }
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "item = {'a': 1, 'b': 'asdfasdf'}\n",
    "with jsonlines.open('../data/output.jsonl', 'a') as writer:\n",
    "#     writer.write_all(items)\n",
    "    writer.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T18:40:33.567594Z",
     "iopub.status.busy": "2021-07-16T18:40:33.567324Z",
     "iopub.status.idle": "2021-07-16T18:40:33.571085Z",
     "shell.execute_reply": "2021-07-16T18:40:33.570567Z",
     "shell.execute_reply.started": "2021-07-16T18:40:33.567567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 'asdfasdf'}\n",
      "{'a': 1, 'b': 'asdfasdf'}\n",
      "{'a': 1, 'b': 'asdfasdf'}\n",
      "{'a': 1, 'b': 'asdfasdf'}\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open('../data/output.jsonl') as reader:\n",
    "#     writer.write_all(items)\n",
    "    for item in reader:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T18:40:43.061442Z",
     "iopub.status.busy": "2021-07-16T18:40:43.061290Z",
     "iopub.status.idle": "2021-07-16T18:40:43.068311Z",
     "shell.execute_reply": "2021-07-16T18:40:43.067805Z",
     "shell.execute_reply.started": "2021-07-16T18:40:43.061426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'70708ecc-19cb-4815-ac44-46d0f0fe72cb'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T18:41:41.793237Z",
     "iopub.status.busy": "2021-07-16T18:41:41.793067Z",
     "iopub.status.idle": "2021-07-16T18:41:41.796272Z",
     "shell.execute_reply": "2021-07-16T18:41:41.795794Z",
     "shell.execute_reply.started": "2021-07-16T18:41:41.793214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-07-16T18:41:41.793804'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.utcnow().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T18:42:03.508194Z",
     "iopub.status.busy": "2021-07-16T18:42:03.508033Z",
     "iopub.status.idle": "2021-07-16T18:42:03.527982Z",
     "shell.execute_reply": "2021-07-16T18:42:03.527335Z",
     "shell.execute_reply.started": "2021-07-16T18:42:03.508178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved [op: \"train1\", job: \"job 2\", id: 2904ba5d-f1c6-459d-a26e-95fae479f5bf] result into \"../data//train1.jsonl\"\n",
      "2904ba5d-f1c6-459d-a26e-95fae479f5bf\n",
      "Saved [op: \"train1\", job: \"job 2\", id: 78bf29ce-523f-4917-b7f5-05b386bf9be4] result into \"../data//train1.jsonl\"\n",
      "78bf29ce-523f-4917-b7f5-05b386bf9be4\n",
      "Saved [op: \"train1\", job: \"job 2\", id: dc1c208d-70e1-46c4-8422-fb833f00be45] result into \"../data//train1.jsonl\"\n",
      "dc1c208d-70e1-46c4-8422-fb833f00be45\n",
      "Saved [op: \"train1\", job: \"job 2\", id: 4a7267aa-c4b4-413a-b9e0-a880c47af2a6] result into \"../data//train1.jsonl\"\n",
      "4a7267aa-c4b4-413a-b9e0-a880c47af2a6\n",
      "Saved [op: \"train1\", job: \"job 2\", id: 7e862313-f4f9-445a-b6ab-aa4720a13a55] result into \"../data//train1.jsonl\"\n",
      "7e862313-f4f9-445a-b6ab-aa4720a13a55\n",
      "3\n",
      "Saved [op: \"train1\", job: \"job 2\", id: 92cab92c-aea1-4715-a0a0-6b8e71313437] result into \"../data//train1.jsonl\"\n",
      "92cab92c-aea1-4715-a0a0-6b8e71313437\n",
      "3\n",
      "[{'id': '4a7267aa-c4b4-413a-b9e0-a880c47af2a6', 'time': '2021-07-16T18:42:03.524152', 'job': 'job 2', 'result': {'a': 1, 'b': 'asdfasdf'}}, {'id': '7e862313-f4f9-445a-b6ab-aa4720a13a55', 'time': '2021-07-16T18:42:03.524622', 'job': 'job 2', 'result': {'a': 1, 'b': 'asdfasdf'}}, {'id': '92cab92c-aea1-4715-a0a0-6b8e71313437', 'time': '2021-07-16T18:42:03.525233', 'job': 'job 2', 'result': {'a': 1, 'b': 'asdfasdf'}}]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "history_retention_number = 3\n",
    "\n",
    "def get_file_name(op):\n",
    "    return f'{data_dir}/{op}.jsonl'\n",
    "    \n",
    "def save_json_in_line(op: str, job: str, js: dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    file_name = get_file_name(op)\n",
    "    id  = str(uuid.uuid4())\n",
    "    new_item = {\n",
    "        'id': id,\n",
    "        'time': datetime.utcnow().isoformat(),\n",
    "        'job': job,\n",
    "        'result': js\n",
    "    }\n",
    "    \n",
    "    existed_items = read_id_json(op)\n",
    "    if len(existed_items) >= history_retention_number:\n",
    "        # remove first (oldest) element\n",
    "        existed_items = existed_items[1:]\n",
    "\n",
    "    with jsonlines.open(file_name, 'w') as writer:\n",
    "        for item in existed_items + [new_item]:\n",
    "            writer.write(item)\n",
    "    print(f'Saved [op: \"{op}\", job: \"{job}\", id: {id}] result into \"{file_name}\"')\n",
    "    return id\n",
    "\n",
    "def read_id_json(op: str, id: str = None):\n",
    "    file_name = get_file_name(op)\n",
    "    if not os.path.exists(file_name):\n",
    "        return []\n",
    "    with jsonlines.open(file_name) as reader:\n",
    "        if not id:\n",
    "            return list(reader)\n",
    "        else:\n",
    "            for js in reader:\n",
    "                if str(js['id']) == id:\n",
    "                    return js\n",
    "            return []\n",
    "\n",
    "def read_job_json(op: str, job: str = None):\n",
    "    file_name = get_file_name(op)\n",
    "    if not os.path.exists(file_name):\n",
    "        return []\n",
    "    with jsonlines.open(file_name) as reader:\n",
    "        if not job:\n",
    "            return list(reader)\n",
    "        else:\n",
    "            return [el for el in reader if el['job'] == job]\n",
    "\n",
    "data_dir = '../data/'\n",
    "op = 'train1'\n",
    "js = {'a': 1, 'b': 'asdfasdf'}\n",
    "job = 'job 2'\n",
    "# id = save_json_in_line(op, job, js)\n",
    "# id = save_json_in_line(op, job, js)\n",
    "# id = save_json_in_line(op, job, js)\n",
    "# print(read_id_json(op, id))\n",
    "# print(read_id_json(op, None))\n",
    "# print(read_id_json(op, 'id'))\n",
    "# print(read_id_json('op', 'id'))\n",
    "# print()\n",
    "# print(read_job_json(op, job))\n",
    "# print(read_job_json(op, None))\n",
    "# print(read_job_json(op, 'id'))\n",
    "# print(read_job_json('op', 'id'))\n",
    "\n",
    "print(save_json_in_line(op, job, js))\n",
    "print(save_json_in_line(op, job, js))\n",
    "print(save_json_in_line(op, job, js))\n",
    "print(save_json_in_line(op, job, js))\n",
    "print(save_json_in_line(op, job, js))\n",
    "print(len(read_id_json(op)))\n",
    "print(save_json_in_line(op, job, js))\n",
    "print(len(read_id_json(op)))\n",
    "print((read_id_json(op)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T18:42:15.561800Z",
     "iopub.status.busy": "2021-07-16T18:42:15.561538Z",
     "iopub.status.idle": "2021-07-16T18:42:15.565706Z",
     "shell.execute_reply": "2021-07-16T18:42:15.565218Z",
     "shell.execute_reply.started": "2021-07-16T18:42:15.561772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7b79965f-5f68-4dc2-80ba-419359221541\n"
     ]
    }
   ],
   "source": [
    "history_retention_number = 30\n",
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "a = []\n",
    "a[-history_retention_number:]\n",
    "len(\"4b28397b-16d1-4036-a6ff-0b685fdac9c8\"), len('99ac257f-bbd1-4b67-bd5b-3e36be02cbab')\n",
    "print(str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T18:42:26.120497Z",
     "iopub.status.busy": "2021-07-16T18:42:26.120092Z",
     "iopub.status.idle": "2021-07-16T18:42:26.144543Z",
     "shell.execute_reply": "2021-07-16T18:42:26.144054Z",
     "shell.execute_reply.started": "2021-07-16T18:42:26.120451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bb196119-d982-4a6a-8c5b-eba1d2c60a22',\n",
       " 'test_f8754e9c-d9a9-4a4b-a999-488d6f76f6b2',\n",
       " 'test_f8754e9c-d9a9-4a4b-a999-488d6f76f6b2',\n",
       " 'test_af4dca4c-64a2-4bc6-ae70-fcf8eb236969',\n",
       " 'test_af4dca4c-64a2-4bc6-ae70-fcf8eb236969',\n",
       " 'test_af4dca4c-64a2-4bc6-ae70-fcf8eb236969',\n",
       " 'test_af4dca4c-64a2-4bc6-ae70-fcf8eb236969']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_file_name(op):\n",
    "    return f'{data_dir}/{op}.jsonl'\n",
    "\n",
    "def read_id_json(op: str, id: str = '-1'):\n",
    "    \"\"\"\n",
    "    Read a json from a file.\n",
    "    @param op: an operation, saved as a file name part. It means several independent files, one file per operation.\n",
    "    @param id: id of the previously saved dictionary.\n",
    "    @return: a dictionary or a List[dict] if id is None. Can be [] if didn't find any.\n",
    "    \"\"\"\n",
    "    file_name = get_file_name(op)\n",
    "    if not os.path.exists(file_name):\n",
    "        return []\n",
    "    # if id is an int, we assume id < 0 (a record index bach from the tail of the list)\n",
    "    # any other non-int id we treat as the uuid.\n",
    "    try:\n",
    "        back_id = int(id)\n",
    "        if back_id >= 0:\n",
    "            back_id = -1\n",
    "    except (ValueError, TypeError):\n",
    "        back_id = -1\n",
    "    with jsonlines.open(file_name) as reader:\n",
    "        records = list(reader)\n",
    "    for js in records:\n",
    "        if str(js['id']) == id:\n",
    "            return js\n",
    "    return records[back_id]\n",
    "\n",
    "data_dir = '../../performance_hotspots/data'\n",
    "\n",
    "tsts = [-3, -2, '-2', 0, '2', 'adadsfsdf', '4f24127d-1d87-402c-b6e2-5aa951b1f9e1']\n",
    "[read_id_json('self_diagnostics', id=id)['id'] for id in tsts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T18:42:39.559032Z",
     "iopub.status.busy": "2021-07-16T18:42:39.558754Z",
     "iopub.status.idle": "2021-07-16T18:42:39.570088Z",
     "shell.execute_reply": "2021-07-16T18:42:39.569492Z",
     "shell.execute_reply.started": "2021-07-16T18:42:39.559005Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Job = namedtuple('Job', 'name params field model_name  data_type source_log group_fields tolerance dynamic_model')\n",
    "jobs = [\n",
    "    Job('port_scan', {'PH_port_scan_threshold': 500},\n",
    "        'hits.hits._source.dest_port', 'PortScanModel', 'source', 'flows',\n",
    "        ['start_time', 'source_namespace', 'source_name_aggr'], 0.6, 1),\n",
    "    Job('ip_sweep', {'PH_ip_sweep_threshold': 32},\n",
    "        'hits.hits._source.dest_ip', 'IpSweepModel', 'source', 'flows',\n",
    "        ['start_time', 'source_namespace', 'source_name_aggr'], 0.6, 1),\n",
    "]\n",
    "job_name2job = {job.name: job for job in jobs}\n",
    "\n",
    "def get_param(job, param_name, param_type=str):\n",
    "    assert param_type in [str, int, float, bool]\n",
    "    default_param = job_name2job[job].params[param_name]\n",
    "    val = os.getenv(param_name, default_param)\n",
    "    if param_type == str:\n",
    "        val = str(val)\n",
    "    elif param_type == int:\n",
    "        val = int(val)\n",
    "    elif param_type == float:\n",
    "        val = float(val)\n",
    "    elif param_type == bool:\n",
    "        val = eval(val)\n",
    "    return val\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "params: the env vars that only can be used outside for configuration with API.\n",
    "data_type: is not always the source_log, because we can aggregate some log data. For example, 'source' and 'dest' types\n",
    "produced from the 'flows' log. \n",
    "\"\"\"\n",
    "\n",
    "job = 'port_scan'\n",
    "param_name = 'PH_port_scan_threshold'\n",
    "\n",
    "os.environ[param_name] = str(600)\n",
    "ret = get_param(job, param_name, param_type=str)\n",
    "assert ret == '600'\n",
    "\n",
    "os.environ[param_name] = str(700)\n",
    "ret = get_param(job, param_name, param_type=int)\n",
    "assert ret == 700\n",
    "\n",
    "os.environ[param_name] = str(1.1)\n",
    "ret = get_param(job, param_name, param_type=float)\n",
    "assert ret == 1.1\n",
    "\n",
    "os.environ[param_name] = 'True'\n",
    "ret = get_param(job, param_name, param_type=bool)\n",
    "assert ret == True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data preprocessing cycle started with saving the downloaded log data into the temporary storage.\n",
    "\n",
    "Then we read stored log data from the several previous downloads to get a whole time series for detection (or training).\n",
    "\n",
    "We write the downloaded data without any conditions. So, the files could keep duplicate data. We add a compress func to\n",
    "save some storage space.\n",
    "\n",
    "We read data in a backward sequence, a file after file, starting from the last file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Storage structure:\n",
    "    for log in logs: # data/logs/\n",
    "        [<log>.<first_start_time>__<last_start_time>.csv]\n",
    "\n",
    "Operations:\n",
    "    read(log, back_index=-1) \n",
    "    # returns a df from a file with back_index in the written file sequence. The sequence\n",
    "    # based on the start_time of the first row of a file, effectively it is a sequence of the written times.\n",
    "    # we can read a file by file in this sequence back in time, starting from -1. -1, -2, -3, etc.\n",
    "    write(log, data) # -> <log>.<first_start_time>__<last_start_time>.csv, \n",
    "        # TBD It could join overlapping files (the last data prevails)\n",
    "    compress(log, max_records=TBD, total_max_records=TBD) \n",
    "        # This operation executed periodically to clean up the space.\n",
    "        # It joins the overlapping files, the last in time file keeps all data, \n",
    "        # the previous overapped file keeps only non-overlapped data. Both (or more) files join into a single file.\n",
    "        # max_records defines the max size of the joined file.\n",
    "        # total_max_records defines the total size of the storage. All old log files that are not fit into this amount \n",
    "        # are removed.\n",
    "    \n",
    "Helpers: TBD\n",
    "    get_uninterrupted_dt_intervals(log, start_time, end_time)\n",
    "    get_spans(log) \n",
    "        # returns [[start_time, end_time],..] - spans, interruptions in dt-s. \n",
    "        # Use it to request additional downloads, to remove the interruption spans.\n",
    "    get_samples(log, samples_num, sample_size=..., last_sample_end_time=None) \n",
    "        # sample_size in the interval_bucket_size. say `5min`\n",
    "        # last_sample_end_time: None means 'samples back from now'\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T23:19:55.170638Z",
     "iopub.status.busy": "2021-07-20T23:19:55.169901Z",
     "iopub.status.idle": "2021-07-20T23:19:55.188663Z",
     "shell.execute_reply": "2021-07-20T23:19:55.188049Z",
     "shell.execute_reply.started": "2021-07-20T23:19:55.170564Z"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import List, Optional, DefaultDict, Union\n",
    "import dateutil\n",
    "from pathlib import Path\n",
    "\n",
    "class Log(Enum):\n",
    "    flows = 'flows'\n",
    "    dns = 'dns'\n",
    "    l7 = 'l7'\n",
    "    \n",
    "    \n",
    "class TimeSeriesStorage:\n",
    "    def __init__(self, data_dir: str ='../data/logs'):\n",
    "        self.data_dir = data_dir\n",
    "        self.dt_field = 'start_time' \n",
    "        \n",
    "    def read(self, log: Log, back_index: int = -1) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        returns a df from a file with back_index in the written file sequence. The sequence\n",
    "        based on the start_time of the first row of a file, effectively it is a sequence of the written times.\n",
    "        we can read a file by file in this sequence back in time, starting from -1. -1, -2, -3, etc.\n",
    "        Returns:\n",
    "            pd.DataFrame\n",
    "            a name of the read file\n",
    "        \"\"\"\n",
    "        pattern = f'{self.data_dir}/{log.name}.*__*.csv' # dns.20210322_170023__20210322_170053.csv\n",
    "        file_pathes = sorted(list(glob.glob(pattern)))\n",
    "#         print(file_pathes)\n",
    "        if not file_pathes or len(file_pathes) <= abs(back_index):\n",
    "            return None\n",
    "        file_name = file_pathes[back_index]\n",
    "        df = pd.read_csv(file_name)\n",
    "        print(f'Loaded {df.shape} \"{file_name}\"')\n",
    "        return df, file_name\n",
    "    \n",
    "    def write(self, log: str, data: List[dict]) -> None:\n",
    "        \"\"\"\n",
    "        Validates if all data records have the 'start_time' field.\n",
    "        Sorts data in a file by the 'start_time' field, ascending.\n",
    "        Writes data into a file like this 'data/logs/dns.20210322_162851__20210322_192851.csv',\n",
    "         where the file name is <log>.<first_start_time>__<last_start_time>.csv\n",
    "        Returns:\n",
    "            a name of the written file\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "        self._assert_data(data)\n",
    "        tmp = sorted([el[self.dt_field] for el in data])\n",
    "        dt_first = tmp[0]\n",
    "        dt_last = tmp[-1]        \n",
    "        file_name = self._get_file_name(log, dt_first, dt_last)\n",
    "        pd.DataFrame(data).sort_values(by=self.dt_field).to_csv(file_name, index=False)\n",
    "        print(f'Saved {len(data):,} \"{file_name}\".')\n",
    "        return file_name\n",
    "     \n",
    "    def compress(log, file_max_records=1000000, total_max_records=10000000):\n",
    "        # This operation executed periodically to clean up the space.\n",
    "        # It joins the overlapping files, the last in time file keeps all data, \n",
    "        # the previous overapped file keeps only non-overlapped data. Both (or more) files join into a single file.\n",
    "        # file_max_records defines the max size of the joined file.\n",
    "        # total_max_records defines the total size of the storage. All old log files that are not fit into this amount \n",
    "        # are removed. Effectively, it is a retention policy.\n",
    "        def flush_concatenated():\n",
    "            if concatenated_df is not None:\n",
    "                self.write(log, concatenated_df.to_dict(orient='records'))\n",
    "                concatenated_df = None\n",
    "                # remove all concatenated files\n",
    "                _ = [os.remove(f) if os.path.exists(f) else None for f in read_files]\n",
    "                read_files = []\n",
    "\n",
    "        pattern = f'{self.data_dir}/{log.name}.*__*.csv' # dns.20210322_170023__20210322_170053.csv\n",
    "        file_pathes = sorted(list(glob.glob(pattern)))\n",
    "        \n",
    "        concatenated_df = None\n",
    "        read_files = []\n",
    "        for i, file in enumerate(file_pathes[::-1]): # from the last to the first\n",
    "            df, read_file = self.read(log, back_index=-(i+1))\n",
    "            if df.shape[0] >= file_max_records: # leave big file as is\n",
    "                flush_concatenated() # and flush, even if the concatenated is small.\n",
    "                continue\n",
    "            if concatenated_df is not None and concatenated_df.shape[0] + df.shape[0] >= file_max_records:\n",
    "                flush_concatenated() \n",
    "            read_files.append(read_file)\n",
    "            concatenated_df = pd.concat([concatenated_df, df]).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "    def _assert_data(self, data: List[dict]) -> bool:\n",
    "        # all records must have a dt_field\n",
    "        for rec in data:\n",
    "            assert type(rec) == dict, f'type of rec should be \"dict\" but was {type(rec)}. rec: {rec}'\n",
    "            assert self.dt_field in rec, f'A record must have the \"{self.dt_field}\" but it has {rec.keys()} fields only.'\n",
    "        return\n",
    "\n",
    "    def _get_file_name(self, prefix: str, start_time: Union[int, str], end_time: Union[int, str], extention: str = 'csv') -> str:\n",
    "        return Path(f'{self.data_dir}/{prefix}.{get_dt_str(start_time)}__{get_dt_str(end_time)}.{extention}')\n",
    "    \n",
    "def get_dt_str(dt):\n",
    "    # dns: 2021-04-19T15:44:44.521995987Z\n",
    "    # l7: 1618865429\n",
    "    # flows: 1618847092\n",
    "    if type(dt) == str:\n",
    "        t = datetime.strptime(dt.split('.')[0], '%Y-%m-%dT%H:%M:%S')\n",
    "    elif type(dt) == int:\n",
    "        t = datetime.fromtimestamp(dt)\n",
    "    return t.strftime('%Y%m%d_%H%M%S') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T02:53:57.559234Z",
     "iopub.status.busy": "2021-07-20T02:53:57.559065Z",
     "iopub.status.idle": "2021-07-20T02:53:57.563202Z",
     "shell.execute_reply": "2021-07-20T02:53:57.562406Z",
     "shell.execute_reply.started": "2021-07-20T02:53:57.559218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20210419_154444', '20210419_135029', '20210419_084452']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tests for get_dt_str\n",
    "tests = ['2021-04-19T15:44:44.521995987Z', 1618865429, 1618847092, ] # '1618847092']\n",
    "[get_dt_str(t) for t in tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T23:19:58.358575Z",
     "iopub.status.busy": "2021-07-20T23:19:58.358383Z",
     "iopub.status.idle": "2021-07-20T23:19:58.488119Z",
     "shell.execute_reply": "2021-07-20T23:19:58.486405Z",
     "shell.execute_reply.started": "2021-07-20T23:19:58.358553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (100, 20) \"../data/logs/dns.20210322_170023__20210322_170053.csv\"\n",
      "Loaded (100, 20) \"../data/logs/dns.20210322_170023__20210322_170053.csv\"\n",
      "Loaded (100, 20) \"../data/logs/dns.20210322_165000__20210322_165027.csv\"\n",
      "Loaded (100, 20) \"../data/logs/dns.20210322_165000__20210322_165027.csv\"\n",
      "Loaded (10, 20) \"../data/logs/dns.20210322_164535__20210322_164535.csv\"\n",
      "Loaded (10, 20) \"../data/logs/dns.20210322_164535__20210322_164535.csv\"\n",
      "Loaded (10, 20) \"../data/logs/dns.20210322_163502__20210322_163502.csv\"\n",
      "Loaded (10, 20) \"../data/logs/dns.20210322_163502__20210322_163502.csv\"\n",
      "Loaded (100, 22) \"../data/logs/l7.20210331_203701__20210331_204229.csv\"\n",
      "Loaded (100, 22) \"../data/logs/l7.20210331_203701__20210331_204229.csv\"\n",
      "Loaded (100, 22) \"../data/logs/l7.20210331_185148__20210331_190241.csv\"\n",
      "Loaded (100, 22) \"../data/logs/l7.20210331_185148__20210331_190241.csv\"\n",
      "Loaded (10, 22) \"../data/logs/l7.20210331_182605__20210331_183037.csv\"\n",
      "Loaded (10, 22) \"../data/logs/l7.20210331_182605__20210331_183037.csv\"\n",
      "Loaded (10, 22) \"../data/logs/l7.20210331_170129__20210331_170252.csv\"\n",
      "Loaded (10, 22) \"../data/logs/l7.20210331_170129__20210331_170252.csv\"\n",
      "Loaded (100, 40) \"../data/logs/flows.20210304_120402__20210304_120953.csv\"\n",
      "Loaded (100, 40) \"../data/logs/flows.20210304_120402__20210304_120953.csv\"\n",
      "Loaded (100, 40) \"../data/logs/flows.20210304_103913__20210304_104606.csv\"\n",
      "Loaded (100, 40) \"../data/logs/flows.20210304_103913__20210304_104606.csv\"\n",
      "Loaded (10, 40) \"../data/logs/flows.20210304_102303__20210304_102303.csv\"\n",
      "Loaded (10, 40) \"../data/logs/flows.20210304_102303__20210304_102303.csv\"\n",
      "Loaded (10, 40) \"../data/logs/flows.20210304_091518__20210304_091522.csv\"\n",
      "Loaded (10, 40) \"../data/logs/flows.20210304_091518__20210304_091522.csv\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(100, 20),\n",
       " (100, 20),\n",
       " (10, 20),\n",
       " (10, 20),\n",
       " 'None',\n",
       " 'None',\n",
       " (100, 22),\n",
       " (100, 22),\n",
       " (10, 22),\n",
       " (10, 22),\n",
       " 'None',\n",
       " 'None',\n",
       " (100, 40),\n",
       " (100, 40),\n",
       " (10, 40),\n",
       " (10, 40),\n",
       " 'None',\n",
       " 'None']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test read()\n",
    "import glob\n",
    "\n",
    "\n",
    "cls = TimeSeriesStorage()\n",
    "logs = [Log.dns, Log.l7, Log.flows]\n",
    "[cls.read(log, back_index=-i).shape if cls.read(log, back_index=-i) is not None else \"None\" for log in logs for i in range(1, 7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-21T00:04:20.668655Z",
     "iopub.status.busy": "2021-07-21T00:04:20.668463Z",
     "iopub.status.idle": "2021-07-21T00:04:20.679664Z",
     "shell.execute_reply": "2021-07-21T00:04:20.679256Z",
     "shell.execute_reply.started": "2021-07-21T00:04:20.668638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0   1   2\n",
      "1000  1   2   3\n",
      "2000  1   5   6\n",
      "5000  1  12  13\n",
      "   0  1  2\n",
      "1  1  2  3\n",
      "2  4  5  6\n",
      "3  7  8  9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2\n",
       "1000  1   2   3\n",
       "2000  1   5   6\n",
       "5000  1  12  13"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates in concatenation:\n",
    "df1 = pd.DataFrame([[1,2,3],[1,5,6],[1,12,13]], index=[1000,2000,5000])\n",
    "print(df1)\n",
    "df2 = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], index=[1,2,3])\n",
    "print(df2)\n",
    "\n",
    "pd.concat([df1,df2]).drop_duplicates().reset_index(drop=True)\n",
    "pd.concat([None, df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T02:53:58.476571Z",
     "iopub.status.busy": "2021-07-20T02:53:58.475974Z",
     "iopub.status.idle": "2021-07-20T02:54:16.201223Z",
     "shell.execute_reply": "2021-07-20T02:54:16.200645Z",
     "shell.execute_reply.started": "2021-07-20T02:53:58.476504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonid/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3347: DtypeWarning: Columns (2,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    ('l7', 'cluster.l7.7K.20210402.csv'),\n",
    "    ('dns', 'cluster.dns.202K.20210402.csv'), \n",
    "    ('flows', 'cluster.flows.500K.20210402.csv'), \n",
    "]\n",
    "\n",
    "recs = {log: pd.read_csv(f'../data/{file}').to_dict(orient='records')\n",
    "    for log, file in files\n",
    "}\n",
    "# [rr[0] for rr in recs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T03:14:42.016243Z",
     "iopub.status.busy": "2021-07-20T03:14:42.016076Z",
     "iopub.status.idle": "2021-07-20T03:14:42.030251Z",
     "shell.execute_reply": "2021-07-20T03:14:42.029697Z",
     "shell.execute_reply.started": "2021-07-20T03:14:42.016226Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read \"../data/logs/index.csv\" (1, 3).\n",
      "Read \"../data/logs/index.csv\" (1, 3).\n",
      "Deleted \"../data/logs/index.csv\".\n",
      "Write \"../data/logs/index.csv\" (1, 2).\n",
      "Read \"../data/logs/index.csv\" (1, 3).\n",
      "   Unnamed: 0  a  b\n",
      "0           0  1  2\n",
      "Deleted \"../data/logs/index.csv\".\n",
      "Write \"../data/logs/index.csv\" (1, 3).\n"
     ]
    }
   ],
   "source": [
    "# test index file:\n",
    "def test_index_operations(cls_time_series_storage, test_df):\n",
    "    # cache the original index:\n",
    "    cls_time_series_storage._index_read()\n",
    "    tmp_df = cls_time_series_storage.index_df\n",
    "    \n",
    "    # delete\n",
    "    cls_time_series_storage._index_delete()\n",
    "    assert not os.path.exists(cls_time_series_storage._index_file)\n",
    "    \n",
    "    # write\n",
    "    cls_time_series_storage.index_df = test_df\n",
    "    cls_time_series_storage._index_write()\n",
    "    assert os.path.exists(cls_time_series_storage._index_file)\n",
    "#     return\n",
    "    \n",
    "    # read\n",
    "    cls_time_series_storage.index_df = None\n",
    "    cls_time_series_storage._index_read()\n",
    "    assert cls_time_series_storage.index_df is not None\n",
    "    print(cls_time_series_storage.index_df.head())\n",
    "    if cls_time_series_storage.index_df is not None and tmp_df is not None:\n",
    "        assert cls_time_series_storage.index_df.shape == tmp_df.shape, f'{cls_time_series_storage.index_df.shape} should be {tmp_df.shape}'\n",
    "    \n",
    "    \n",
    "    # delete\n",
    "    cls_time_series_storage._index_delete()\n",
    "    assert not os.path.exists(cls_time_series_storage._index_file)\n",
    "    \n",
    "    # restore the original index\n",
    "    cls_time_series_storage.index_df = tmp_df\n",
    "    cls_time_series_storage._index_write()\n",
    "    if tmp_df is not None:\n",
    "        assert os.path.exists(cls_time_series_storage._index_file)\n",
    "\n",
    "cls_time_series_storage = TimeSeriesStorage()\n",
    "test_df = pd.DataFrame({'a': [1], 'b': [2]})\n",
    "test_index_operations(cls_time_series_storage, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T02:48:47.134539Z",
     "iopub.status.busy": "2021-07-20T02:48:47.134289Z",
     "iopub.status.idle": "2021-07-20T02:48:47.139334Z",
     "shell.execute_reply": "2021-07-20T02:48:47.138537Z",
     "shell.execute_reply.started": "2021-07-20T02:48:47.134509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.DataFrame({'a': [1], 'b': [2]})\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:07:05.292390Z",
     "iopub.status.busy": "2021-07-20T22:07:05.292090Z",
     "iopub.status.idle": "2021-07-20T22:07:05.335798Z",
     "shell.execute_reply": "2021-07-20T22:07:05.335022Z",
     "shell.execute_reply.started": "2021-07-20T22:07:05.292359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 \"../data/logs/l7.20210331_203701__20210331_204229.csv\".\n",
      "1617248549\n",
      "Saved 100 \"../data/logs/dns.20210322_170023__20210322_170053.csv\".\n",
      "2021-03-22T17:00:53.021419666Z\n",
      "Saved 100 \"../data/logs/flows.20210304_120402__20210304_120953.csv\".\n",
      "1614888593\n"
     ]
    }
   ],
   "source": [
    "# test for write:\n",
    "def test_write(cls_time_series_storage, recs, shift, size= 100):\n",
    "    for log, rr in recs.items():\n",
    "        last_dt = cls_time_series_storage.write(log, rr[shift:shift+size])\n",
    "        print(last_dt)\n",
    "        \n",
    "cls_time_series_storage = TimeSeriesStorage()\n",
    "shift = 2500\n",
    "test_write(cls_time_series_storage, recs, shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-17T01:04:24.340908Z",
     "iopub.status.busy": "2021-07-17T01:04:24.340749Z",
     "iopub.status.idle": "2021-07-17T01:04:24.348680Z",
     "shell.execute_reply": "2021-07-17T01:04:24.347724Z",
     "shell.execute_reply.started": "2021-07-17T01:04:24.340892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l7': [{'start_time': 1617231868,\n",
       "   'end_time': 1617232184,\n",
       "   'duration_mean': 701815895,\n",
       "   'duration_max': 2569000000,\n",
       "   'bytes_in': 165998,\n",
       "   'bytes_out': 227789,\n",
       "   'count': 994,\n",
       "   'src_name_aggr': 'frontend-8cbdb8d46-*',\n",
       "   'src_namespace': 'default',\n",
       "   'src_type': 'wep',\n",
       "   'dest_service_name': 'checkoutservice',\n",
       "   'dest_service_namespace': 'default',\n",
       "   'dest_service_port': 5050,\n",
       "   'dest_name_aggr': 'checkoutservice-57f57d4df8-*',\n",
       "   'dest_namespace': 'default',\n",
       "   'dest_type': 'wep',\n",
       "   'method': 'POST',\n",
       "   'user_agent': '-',\n",
       "   'url': 'checkoutservice:5050/hipstershop.CheckoutService/PlaceOrder',\n",
       "   'response_code': 200,\n",
       "   'type': '-',\n",
       "   'host': 'leonid-bz-rezf-kadm-node-1'},\n",
       "  {'start_time': 1617231868,\n",
       "   'end_time': 1617232184,\n",
       "   'duration_mean': 5003958,\n",
       "   'duration_max': 101000000,\n",
       "   'bytes_in': 85371,\n",
       "   'bytes_out': 0,\n",
       "   'count': 5558,\n",
       "   'src_name_aggr': 'frontend-8cbdb8d46-*',\n",
       "   'src_namespace': 'default',\n",
       "   'src_type': 'wep',\n",
       "   'dest_service_name': 'adservice',\n",
       "   'dest_service_namespace': 'default',\n",
       "   'dest_service_port': 9555,\n",
       "   'dest_name_aggr': 'adservice-744544f54d-*',\n",
       "   'dest_namespace': 'default',\n",
       "   'dest_type': 'wep',\n",
       "   'method': 'POST',\n",
       "   'user_agent': '-',\n",
       "   'url': 'adservice:9555/hipstershop.AdService/GetAds',\n",
       "   'response_code': 0,\n",
       "   'type': '-',\n",
       "   'host': 'leonid-bz-rezf-kadm-node-1'},\n",
       "  {'start_time': 1617231868,\n",
       "   'end_time': 1617232184,\n",
       "   'duration_mean': 18800397,\n",
       "   'duration_max': 116000000,\n",
       "   'bytes_in': 154521,\n",
       "   'bytes_out': 1609554,\n",
       "   'count': 9569,\n",
       "   'src_name_aggr': 'frontend-8cbdb8d46-*',\n",
       "   'src_namespace': 'default',\n",
       "   'src_type': 'wep',\n",
       "   'dest_service_name': 'adservice',\n",
       "   'dest_service_namespace': 'default',\n",
       "   'dest_service_port': 9555,\n",
       "   'dest_name_aggr': 'adservice-744544f54d-*',\n",
       "   'dest_namespace': 'default',\n",
       "   'dest_type': 'wep',\n",
       "   'method': 'POST',\n",
       "   'user_agent': '-',\n",
       "   'url': 'adservice:9555/hipstershop.AdService/GetAds',\n",
       "   'response_code': 200,\n",
       "   'type': '-',\n",
       "   'host': 'leonid-bz-rezf-kadm-node-1'}],\n",
       " 'dns': [{'start_time': '2021-03-22T16:28:51.565759887Z',\n",
       "   'end_time': '2021-03-22T16:34:16.228614279Z',\n",
       "   'type': 'log',\n",
       "   'count': 324,\n",
       "   'client_name': '-',\n",
       "   'client_name_aggr': 'loadgenerator-5d4c9686fd-*',\n",
       "   'client_namespace': 'default',\n",
       "   'client_ip': nan,\n",
       "   'client_labels': \"{'app': 'loadgenerator', 'pod-template-hash': '5d4c9686fd'}\",\n",
       "   'servers': \"[{'name': 'coredns-f9fd979d6-npsf7', 'name_aggr': 'coredns-f9fd979d6-*', 'namespace': 'kube-system', 'ip': '192.168.93.36'}, {'name': 'coredns-f9fd979d6-znjgn', 'name_aggr': 'coredns-f9fd979d6-*', 'namespace': 'kube-system', 'ip': '192.168.15.178'}]\",\n",
       "   'qname': 'frontend.default.svc.cluster.local',\n",
       "   'qclass': 'IN',\n",
       "   'qtype': 'A',\n",
       "   'rcode': 'NoError',\n",
       "   'rrsets': \"[{'name': 'frontend.default.svc.cluster.local', 'class': 'IN', 'type': 'A', 'rdata': ['10.106.193.125']}]\",\n",
       "   'latency': \"{'count': 323, 'mean': 751987, 'max': 4905000}\",\n",
       "   'latency_count': 323,\n",
       "   'latency_mean': 751987,\n",
       "   'latency_max': 4905000,\n",
       "   'host': 'leonid-bz-rezf-kadm-node-1'},\n",
       "  {'start_time': '2021-03-22T16:28:51.565759887Z',\n",
       "   'end_time': '2021-03-22T16:34:16.228614279Z',\n",
       "   'type': 'log',\n",
       "   'count': 825,\n",
       "   'client_name': '-',\n",
       "   'client_name_aggr': 'checkoutservice-57f57d4df8-*',\n",
       "   'client_namespace': 'default',\n",
       "   'client_ip': nan,\n",
       "   'client_labels': \"{'app': 'checkoutservice', 'pod-template-hash': '57f57d4df8'}\",\n",
       "   'servers': \"[{'name': 'coredns-f9fd979d6-znjgn', 'name_aggr': 'coredns-f9fd979d6-*', 'namespace': 'kube-system', 'ip': '192.168.15.178'}, {'name': 'coredns-f9fd979d6-npsf7', 'name_aggr': 'coredns-f9fd979d6-*', 'namespace': 'kube-system', 'ip': '192.168.93.36'}]\",\n",
       "   'qname': 'paymentservice.default.svc.cluster.local',\n",
       "   'qclass': 'IN',\n",
       "   'qtype': 'A',\n",
       "   'rcode': 'NoError',\n",
       "   'rrsets': \"[{'name': 'paymentservice.default.svc.cluster.local', 'class': 'IN', 'type': 'A', 'rdata': ['10.101.101.25']}]\",\n",
       "   'latency': \"{'count': 804, 'mean': 848875, 'max': 12547000}\",\n",
       "   'latency_count': 804,\n",
       "   'latency_mean': 848875,\n",
       "   'latency_max': 12547000,\n",
       "   'host': 'leonid-bz-rezf-kadm-node-1'},\n",
       "  {'start_time': '2021-03-22T16:28:51.565759887Z',\n",
       "   'end_time': '2021-03-22T16:34:16.228614279Z',\n",
       "   'type': 'log',\n",
       "   'count': 1646,\n",
       "   'client_name': '-',\n",
       "   'client_name_aggr': 'checkoutservice-57f57d4df8-*',\n",
       "   'client_namespace': 'default',\n",
       "   'client_ip': nan,\n",
       "   'client_labels': \"{'app': 'checkoutservice', 'pod-template-hash': '57f57d4df8'}\",\n",
       "   'servers': \"[{'name': 'coredns-f9fd979d6-npsf7', 'name_aggr': 'coredns-f9fd979d6-*', 'namespace': 'kube-system', 'ip': '192.168.93.36'}, {'name': 'coredns-f9fd979d6-znjgn', 'name_aggr': 'coredns-f9fd979d6-*', 'namespace': 'kube-system', 'ip': '192.168.15.178'}]\",\n",
       "   'qname': 'shippingservice.default.svc.cluster.local',\n",
       "   'qclass': 'IN',\n",
       "   'qtype': 'A',\n",
       "   'rcode': 'NoError',\n",
       "   'rrsets': \"[{'name': 'shippingservice.default.svc.cluster.local', 'class': 'IN', 'type': 'A', 'rdata': ['10.99.44.192']}]\",\n",
       "   'latency': \"{'count': 1604, 'mean': 842442, 'max': 15990000}\",\n",
       "   'latency_count': 1604,\n",
       "   'latency_mean': 842442,\n",
       "   'latency_max': 15990000,\n",
       "   'host': 'leonid-bz-rezf-kadm-node-1'}],\n",
       " 'flows': [{'start_time': 1614876237,\n",
       "   'end_time': 1614876542,\n",
       "   'source_ip': nan,\n",
       "   'source_name': '-',\n",
       "   'source_name_aggr': 'compliance-snapshotter-7bc8cdf468-*',\n",
       "   'source_namespace': 'tigera-compliance',\n",
       "   'source_port': nan,\n",
       "   'source_type': 'wep',\n",
       "   'source_labels': \"{'labels': ['pod-template-hash=7bc8cdf468', 'k8s-app=compliance-snapshotter']}\",\n",
       "   'dest_ip': nan,\n",
       "   'dest_name': '-',\n",
       "   'dest_name_aggr': 'tigera-secure-es-7f5dee596fef130f-*',\n",
       "   'dest_namespace': 'tigera-elasticsearch',\n",
       "   'dest_service_namespace': 'tigera-elasticsearch',\n",
       "   'dest_service_name': 'tigera-secure-es-http',\n",
       "   'dest_service_port': 'https',\n",
       "   'dest_port': 9200,\n",
       "   'dest_type': 'wep',\n",
       "   'dest_labels': \"{'labels': ['elasticsearch.k8s.elastic.co/version=7.10.1', 'elasticsearch.k8s.elastic.co/node-ingest=true', 'controller-revision-hash=tigera-secure-es-7f5dee596fef130f-5c48686b9f', 'elasticsearch.k8s.elastic.co/node-ml=true', 'elasticsearch.k8s.elastic.co/config-hash=3142294097', 'statefulset.kubernetes.io/pod-name=tigera-secure-es-7f5dee596fef130f-0', 'elasticsearch.k8s.elastic.co/node-data=true', 'elasticsearch.k8s.elastic.co/http-scheme=https', 'elasticsearch.k8s.elastic.co/node-master=true', 'elasticsearch.k8s.elastic.co/statefulset-name=tigera-secure-es-7f5dee596fef130f', 'elasticsearch.k8s.elastic.co/cluster-name=tigera-secure', 'common.k8s.elastic.co/type=elasticsearch']}\",\n",
       "   'proto': 'tcp',\n",
       "   'action': 'allow',\n",
       "   'reporter': 'src',\n",
       "   'policies': \"{'all_policies': ['0|allow-tigera|tigera-compliance/allow-tigera.compliance-access|allow']}\",\n",
       "   'bytes_in': 614522,\n",
       "   'bytes_out': 27857,\n",
       "   'num_flows': 14,\n",
       "   'num_flows_started': 14,\n",
       "   'num_flows_completed': 12,\n",
       "   'packets_in': 201,\n",
       "   'packets_out': 201,\n",
       "   'http_requests_allowed_in': 0,\n",
       "   'http_requests_denied_in': 0,\n",
       "   'process_name': '-',\n",
       "   'num_process_names': 0,\n",
       "   'process_id': '-',\n",
       "   'num_process_ids': 0,\n",
       "   'original_source_ips': nan,\n",
       "   'num_original_source_ips': 0,\n",
       "   'host': 'leonid-bz-rezf-kadm-ms',\n",
       "   '@timestamp': 1614876542000},\n",
       "  {'start_time': 1614876237,\n",
       "   'end_time': 1614876542,\n",
       "   'source_ip': nan,\n",
       "   'source_name': '-',\n",
       "   'source_name_aggr': 'fluentd-node-*',\n",
       "   'source_namespace': 'tigera-fluentd',\n",
       "   'source_port': nan,\n",
       "   'source_type': 'wep',\n",
       "   'source_labels': \"{'labels': ['k8s-app=fluentd-node', 'pod-template-generation=1', 'controller-revision-hash=76d9fd56d7']}\",\n",
       "   'dest_ip': nan,\n",
       "   'dest_name': '-',\n",
       "   'dest_name_aggr': 'tigera-secure-es-7f5dee596fef130f-*',\n",
       "   'dest_namespace': 'tigera-elasticsearch',\n",
       "   'dest_service_namespace': 'tigera-elasticsearch',\n",
       "   'dest_service_name': 'tigera-secure-es-http',\n",
       "   'dest_service_port': 'https',\n",
       "   'dest_port': 9200,\n",
       "   'dest_type': 'wep',\n",
       "   'dest_labels': \"{'labels': ['elasticsearch.k8s.elastic.co/node-data=true', 'elasticsearch.k8s.elastic.co/version=7.10.1', 'common.k8s.elastic.co/type=elasticsearch', 'elasticsearch.k8s.elastic.co/node-master=true', 'controller-revision-hash=tigera-secure-es-7f5dee596fef130f-5c48686b9f', 'elasticsearch.k8s.elastic.co/cluster-name=tigera-secure', 'elasticsearch.k8s.elastic.co/node-ingest=true', 'elasticsearch.k8s.elastic.co/statefulset-name=tigera-secure-es-7f5dee596fef130f', 'statefulset.kubernetes.io/pod-name=tigera-secure-es-7f5dee596fef130f-0', 'elasticsearch.k8s.elastic.co/node-ml=true', 'elasticsearch.k8s.elastic.co/http-scheme=https', 'elasticsearch.k8s.elastic.co/config-hash=3142294097']}\",\n",
       "   'proto': 'tcp',\n",
       "   'action': 'allow',\n",
       "   'reporter': 'src',\n",
       "   'policies': \"{'all_policies': ['0|allow-tigera|tigera-fluentd/allow-tigera.allow-fluentd-node|allow']}\",\n",
       "   'bytes_in': 1537166,\n",
       "   'bytes_out': 14038087,\n",
       "   'num_flows': 57,\n",
       "   'num_flows_started': 57,\n",
       "   'num_flows_completed': 57,\n",
       "   'packets_in': 1004,\n",
       "   'packets_out': 1848,\n",
       "   'http_requests_allowed_in': 0,\n",
       "   'http_requests_denied_in': 0,\n",
       "   'process_name': '-',\n",
       "   'num_process_names': 0,\n",
       "   'process_id': '-',\n",
       "   'num_process_ids': 0,\n",
       "   'original_source_ips': nan,\n",
       "   'num_original_source_ips': 0,\n",
       "   'host': 'leonid-bz-rezf-kadm-ms',\n",
       "   '@timestamp': 1614876542000},\n",
       "  {'start_time': 1614876237,\n",
       "   'end_time': 1614876542,\n",
       "   'source_ip': nan,\n",
       "   'source_name': '-',\n",
       "   'source_name_aggr': 'compliance-benchmarker-*',\n",
       "   'source_namespace': 'tigera-compliance',\n",
       "   'source_port': nan,\n",
       "   'source_type': 'wep',\n",
       "   'source_labels': \"{'labels': ['controller-revision-hash=55cd6f6cc', 'k8s-app=compliance-benchmarker', 'pod-template-generation=1']}\",\n",
       "   'dest_ip': nan,\n",
       "   'dest_name': '-',\n",
       "   'dest_name_aggr': 'kse.kubernetes',\n",
       "   'dest_namespace': 'default',\n",
       "   'dest_service_namespace': 'default',\n",
       "   'dest_service_name': 'kubernetes',\n",
       "   'dest_service_port': 'https',\n",
       "   'dest_port': 6443,\n",
       "   'dest_type': 'ns',\n",
       "   'dest_labels': \"{'labels': ['endpoints.projectcalico.org/serviceName=kubernetes', 'provider=kubernetes', 'component=apiserver']}\",\n",
       "   'proto': 'tcp',\n",
       "   'action': 'allow',\n",
       "   'reporter': 'src',\n",
       "   'policies': \"{'all_policies': ['0|allow-tigera|tigera-compliance/allow-tigera.compliance-access|allow']}\",\n",
       "   'bytes_in': 18792,\n",
       "   'bytes_out': 8378,\n",
       "   'num_flows': 1,\n",
       "   'num_flows_started': 1,\n",
       "   'num_flows_completed': 0,\n",
       "   'packets_in': 103,\n",
       "   'packets_out': 100,\n",
       "   'http_requests_allowed_in': 0,\n",
       "   'http_requests_denied_in': 0,\n",
       "   'process_name': '-',\n",
       "   'num_process_names': 0,\n",
       "   'process_id': '-',\n",
       "   'num_process_ids': 0,\n",
       "   'original_source_ips': nan,\n",
       "   'num_original_source_ips': 0,\n",
       "   'host': 'leonid-bz-rezf-kadm-ms',\n",
       "   '@timestamp': 1614876542000}]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
